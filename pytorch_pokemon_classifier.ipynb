{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_pokemon_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBhm-VW1wkK1",
        "outputId": "7c2306f0-3526-4af5-b8f6-5ff8dba10959"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"placeholder-path\"\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTbNljUkwk2M"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "from torch.optim import Adam\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6mkTZ6t7O38"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW5QnA_KxpO6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "9491eb9f-7ad3-43d5-be68-40b488b2b3cf"
      },
      "source": [
        "folder_name = 'PokemonData'\n",
        "model_data = datasets.ImageFolder(folder_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6cbf3f5b7b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PokemonData'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV6SiUg-7WEh"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phEVUVOv7cT3"
      },
      "source": [
        "#Define transformations for the training set and test set\n",
        "\n",
        "train_transformations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_transformations = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miTR8wypOOht"
      },
      "source": [
        "# Randomly split into train, validation, and test sets\n",
        "\n",
        "total_count = len(model_data)\n",
        "train_count = int(0.7 * total_count)\n",
        "valid_count = int(0.2 * total_count)\n",
        "test_count = total_count - train_count - valid_count\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    model_data, (train_count, valid_count, test_count)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V7dpWbQOqGH"
      },
      "source": [
        "# Apply transformations accordingly\n",
        "\n",
        "train_dataset.dataset.transform = train_transformations\n",
        "valid_dataset.dataset.transform = test_transformations\n",
        "test_dataset.dataset.transform = test_transformations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jrMXrOs6Zf"
      },
      "source": [
        "# Load training & validation sets for training\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5trwsVp7x3C"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWw2aAsLyeMr"
      },
      "source": [
        "# Define model architecture\n",
        "def get_model():\n",
        "    model = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, 3, stride=1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Conv2d(64, 128, 3, stride=1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout(p=0.3),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.Conv2d(128, 128, 3, stride=1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Conv2d(128, 256, 3, stride=1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.BatchNorm2d(256),\n",
        "          nn.Conv2d(256, 256, 3, stride=1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout(p=0.3),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(256*8*8, 1024),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.3),\n",
        "          nn.Linear(1024, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 150),\n",
        "          # nn.Softmax(dim=1) # exclude Softmax layer as Torch applies it during prediction\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to adjust learning rate over epoch\n",
        "def adjust_learning_rate(epoch):\n",
        "\n",
        "    lr = 0.001\n",
        "\n",
        "    if epoch > 180:\n",
        "        lr = lr / 1000000\n",
        "    elif epoch > 150:\n",
        "        lr = lr / 100000\n",
        "    elif epoch > 120:\n",
        "        lr = lr / 10000\n",
        "    elif epoch > 90:\n",
        "        lr = lr / 1000\n",
        "    elif epoch > 60:\n",
        "        lr = lr / 100\n",
        "    elif epoch > 30:\n",
        "        lr = lr / 10\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "# Function to get validation accuracy\n",
        "def validation():\n",
        "    model.eval()\n",
        "    valid_acc = 0.0\n",
        "    for i, (images, labels) in enumerate(validation_loader):\n",
        "      \n",
        "        if cuda_avail:\n",
        "              images = images.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "        #Predict classes using images from the test set\n",
        "        outputs = model(images)\n",
        "        _,prediction = torch.max(outputs.data, 1)\n",
        "        valid_acc += torch.sum(prediction == labels.data)\n",
        "        \n",
        "\n",
        "\n",
        "    #Compute the average acc and loss over all 10000 test images\n",
        "    valid_acc = valid_acc / valid_count\n",
        "\n",
        "    return valid_acc\n",
        "  \n",
        "# Function to save models that are better than before\n",
        "def save_models(epoch):\n",
        "    torch.save(model.state_dict(), \"pytorch_model_{}.model\".format(epoch))\n",
        "    print(\"Checkpoint saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrdWxRah81J-"
      },
      "source": [
        "## Define training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce8lrPi9yKsu"
      },
      "source": [
        "# Check if gpu support is available\n",
        "cuda_avail = torch.cuda.is_available()\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "model = get_model()\n",
        "\n",
        "#if cuda is available, move the model to the GPU\n",
        "if cuda_avail:\n",
        "    model.cuda()\n",
        "\n",
        "#Define the optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR-u0tGU88nt"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRstpw-_fl19",
        "outputId": "ca25e563-6fc0-4b8c-abd6-bebb4532ecf9"
      },
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(30):\n",
        "      print(\"epoch: {}\".format(epoch))\n",
        "      model.train()\n",
        "      train_acc = 0.0\n",
        "      train_loss = 0.0\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          #Move images and labels to gpu if available\n",
        "          if cuda_avail:\n",
        "              images = images.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "          #Clear all accumulated gradients\n",
        "          optimizer.zero_grad()\n",
        "          #Predict classes using images from the test set\n",
        "          outputs = model(images)\n",
        "          #Compute the loss based on the predictions and actual labels\n",
        "          loss = loss_fn(outputs,labels)\n",
        "          #Backpropagate the loss\n",
        "          loss.backward()\n",
        "\n",
        "          #Adjust parameters according to the computed gradients\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item() * images.size(0)\n",
        "          _, prediction = torch.max(outputs.data, 1)\n",
        "          \n",
        "          train_acc += torch.sum(prediction == labels.data)\n",
        "        \n",
        "      train_acc = train_acc / train_count * 100\n",
        "      train_loss = train_loss / train_count\n",
        "      print(\"Loss: {}, Training accuracy: {}%\".format(train_loss, train_acc))\n",
        "      print(prediction, labels.data)\n",
        "\n",
        "      #Call the learning rate adjustment function\n",
        "      adjust_learning_rate(epoch)\n",
        "\n",
        "      valid_acc = validation()\n",
        "      print(\"Validation acc: {}&\".format(valid_acc*100))\n",
        "      if valid_acc > best_acc:\n",
        "            save_models(epoch)\n",
        "            best_acc = valid_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "Loss: 0.3984510304463249, Training accuracy: 89.0029296875%\n",
            "tensor([128, 137, 127,  59, 118, 123], device='cuda:0') tensor([128,  61, 101,   2, 118, 123], device='cuda:0')\n",
            "Validation acc: 53.22581100463867&\n",
            "Checkpoint saved\n",
            "epoch: 1\n",
            "Loss: 0.4088521250818201, Training accuracy: 88.60494232177734%\n",
            "tensor([ 21,  35,  41, 102, 143,  36], device='cuda:0') tensor([ 89,  35,  41, 102, 143,  36], device='cuda:0')\n",
            "Validation acc: 54.10557556152344&\n",
            "Checkpoint saved\n",
            "epoch: 2\n",
            "Loss: 0.33191195058753026, Training accuracy: 90.53204345703125%\n",
            "tensor([127, 117,  78,  36, 130,  59], device='cuda:0') tensor([127, 117,  78,  36, 130,  59], device='cuda:0')\n",
            "Validation acc: 47.36070251464844&\n",
            "epoch: 3\n",
            "Loss: 0.40504979467230906, Training accuracy: 89.40092468261719%\n",
            "tensor([136,  89,  34, 130, 135,  62], device='cuda:0') tensor([136,  89, 131, 130, 135,  62], device='cuda:0')\n",
            "Validation acc: 54.10557556152344&\n",
            "epoch: 4\n",
            "Loss: 0.29152639159424737, Training accuracy: 91.80980682373047%\n",
            "tensor([ 91, 125, 120, 134,  91,  45], device='cuda:0') tensor([ 36, 125,  51, 134,  48,  45], device='cuda:0')\n",
            "Validation acc: 52.6392936706543&\n",
            "epoch: 5\n",
            "Loss: 0.35164631696579457, Training accuracy: 90.53204345703125%\n",
            "tensor([ 40,  71, 120, 147,   5, 111], device='cuda:0') tensor([ 87,  71, 120, 147,   5, 111], device='cuda:0')\n",
            "Validation acc: 54.69208526611328&\n",
            "Checkpoint saved\n",
            "epoch: 6\n",
            "Loss: 0.26448452526958643, Training accuracy: 92.91998291015625%\n",
            "tensor([ 43,  62, 143,  44,  22,  88], device='cuda:0') tensor([ 43,  62, 143,  44,  22,  88], device='cuda:0')\n",
            "Validation acc: 53.29912185668945&\n",
            "epoch: 7\n",
            "Loss: 0.3244131574214381, Training accuracy: 91.64222717285156%\n",
            "tensor([  2,  27, 120, 102,  27,  41], device='cuda:0') tensor([  2,  27,   9, 102,  27,  58], device='cuda:0')\n",
            "Validation acc: 53.29912185668945&\n",
            "epoch: 8\n",
            "Loss: 0.3022851195896671, Training accuracy: 91.72601318359375%\n",
            "tensor([132,  81, 141, 125, 118,  84], device='cuda:0') tensor([113,  81, 141, 125, 112,  84], device='cuda:0')\n",
            "Validation acc: 52.93254852294922&\n",
            "epoch: 9\n",
            "Loss: 0.28470356629948373, Training accuracy: 92.2287368774414%\n",
            "tensor([106,  87,  93,  76,  86,  87], device='cuda:0') tensor([106,  87,  94,  76,  86,  87], device='cuda:0')\n",
            "Validation acc: 53.73900604248047&\n",
            "epoch: 10\n",
            "Loss: 0.31060519347679, Training accuracy: 92.08211517333984%\n",
            "tensor([114,  16,  25,  70, 109,  27], device='cuda:0') tensor([87, 16, 22, 70, 78, 27], device='cuda:0')\n",
            "Validation acc: 54.32551574707031&\n",
            "epoch: 11\n",
            "Loss: 0.31053115268960474, Training accuracy: 92.04021453857422%\n",
            "tensor([ 84,  44,  44,  11,   4, 128], device='cuda:0') tensor([142,  44,  44,  11,   4, 128], device='cuda:0')\n",
            "Validation acc: 52.41935348510742&\n",
            "epoch: 12\n",
            "Loss: 0.25079994130751665, Training accuracy: 92.54293823242188%\n",
            "tensor([  5,  32, 119, 131,  25,  76], device='cuda:0') tensor([ 50,  32, 119, 131,  25,  76], device='cuda:0')\n",
            "Validation acc: 52.27272415161133&\n",
            "epoch: 13\n",
            "Loss: 0.24422566763200076, Training accuracy: 93.38081359863281%\n",
            "tensor([ 36, 108,  94,  83, 116, 104], device='cuda:0') tensor([ 36, 108,  94,  83,  86, 104], device='cuda:0')\n",
            "Validation acc: 52.27272415161133&\n",
            "epoch: 14\n",
            "Loss: 0.2445657735980757, Training accuracy: 93.77880096435547%\n",
            "tensor([ 57,  94, 118,  66, 125,  20], device='cuda:0') tensor([ 57,  94, 118,  66, 125,  20], device='cuda:0')\n",
            "Validation acc: 53.44575119018555&\n",
            "epoch: 15\n",
            "Loss: 0.26540870491082064, Training accuracy: 93.464599609375%\n",
            "tensor([21, 24, 60, 90, 35, 88], device='cuda:0') tensor([21, 24, 60, 90, 35, 88], device='cuda:0')\n",
            "Validation acc: 53.005863189697266&\n",
            "epoch: 16\n",
            "Loss: 0.225849782806717, Training accuracy: 94.13489532470703%\n",
            "tensor([138, 125,  21, 117,  75, 102], device='cuda:0') tensor([138,  45,  21, 117,  75, 102], device='cuda:0')\n",
            "Validation acc: 53.812320709228516&\n",
            "epoch: 17\n",
            "Loss: 0.2668235088157408, Training accuracy: 93.4227066040039%\n",
            "tensor([98, 48, 72, 26, 93,  2], device='cuda:0') tensor([98, 48, 72, 26, 93,  2], device='cuda:0')\n",
            "Validation acc: 53.592376708984375&\n",
            "epoch: 18\n",
            "Loss: 0.22724237731315747, Training accuracy: 94.34436798095703%\n",
            "tensor([133, 138,  71,  33,  27,  88], device='cuda:0') tensor([133, 138,  71,  33,  27,  88], device='cuda:0')\n",
            "Validation acc: 54.9853401184082&\n",
            "Checkpoint saved\n",
            "epoch: 19\n",
            "Loss: 0.21496574015767345, Training accuracy: 94.7004623413086%\n",
            "tensor([ 88, 140,  10, 147, 126, 123], device='cuda:0') tensor([ 88, 140,  10, 147, 126, 123], device='cuda:0')\n",
            "Validation acc: 54.17888641357422&\n",
            "epoch: 20\n",
            "Loss: 0.22099397717445798, Training accuracy: 94.15584564208984%\n",
            "tensor([ 77,  83, 136, 122,   3, 112], device='cuda:0') tensor([ 77,  83, 136, 122,   3, 112], device='cuda:0')\n",
            "Validation acc: 51.686214447021484&\n",
            "epoch: 21\n",
            "Loss: 0.2378748352366506, Training accuracy: 94.23963165283203%\n",
            "tensor([ 62,  70,  42, 123,  20, 133], device='cuda:0') tensor([ 62,  70,  42, 123,  31, 133], device='cuda:0')\n",
            "Validation acc: 52.41935348510742&\n",
            "epoch: 22\n",
            "Loss: 0.2421136382823007, Training accuracy: 94.4072036743164%\n",
            "tensor([ 88, 111, 141,  34,  23, 141], device='cuda:0') tensor([ 88, 111, 141,  34,  23, 141], device='cuda:0')\n",
            "Validation acc: 54.03226089477539&\n",
            "epoch: 23\n",
            "Loss: 0.1988802984002062, Training accuracy: 95.3917007446289%\n",
            "tensor([ 59, 131,  66,  65, 111,  63], device='cuda:0') tensor([ 59, 131,  66,  65, 111,  63], device='cuda:0')\n",
            "Validation acc: 54.03226089477539&\n",
            "epoch: 24\n",
            "Loss: 0.23120257926017102, Training accuracy: 94.4490966796875%\n",
            "tensor([ 95,  81, 112, 118,  72, 123], device='cuda:0') tensor([ 95,  81,  65, 118,  72, 123], device='cuda:0')\n",
            "Validation acc: 54.47214126586914&\n",
            "epoch: 25\n",
            "Loss: 0.22903509528410934, Training accuracy: 94.6166763305664%\n",
            "tensor([ 63, 125, 148,  10,  41,  16], device='cuda:0') tensor([ 63, 125, 148,  10, 121,  16], device='cuda:0')\n",
            "Validation acc: 54.03226089477539&\n",
            "epoch: 26\n",
            "Loss: 0.19523402062595324, Training accuracy: 95.22412872314453%\n",
            "tensor([111,  25,  48,  21,   0,  27], device='cuda:0') tensor([111,  25,  48,  21,   0,  27], device='cuda:0')\n",
            "Validation acc: 54.54545593261719&\n",
            "epoch: 27\n",
            "Loss: 0.19747104188776893, Training accuracy: 95.55928039550781%\n",
            "tensor([  0,  73,   5, 123, 146,  51], device='cuda:0') tensor([ 29,  73,   5, 123, 146,  51], device='cuda:0')\n",
            "Validation acc: 54.10557556152344&\n",
            "epoch: 28\n",
            "Loss: 0.2089858204959603, Training accuracy: 94.82614135742188%\n",
            "tensor([ 40, 149, 115,  95,  44, 112], device='cuda:0') tensor([ 40, 149, 115,  95,  44, 112], device='cuda:0')\n",
            "Validation acc: 53.73900604248047&\n",
            "epoch: 29\n",
            "Loss: 0.18314861190705298, Training accuracy: 95.74779510498047%\n",
            "tensor([29, 43, 61, 25, 63, 18], device='cuda:0') tensor([29, 43, 61, 25, 63, 18], device='cuda:0')\n",
            "Validation acc: 54.69208526611328&\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuApqNTnzC8x"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}